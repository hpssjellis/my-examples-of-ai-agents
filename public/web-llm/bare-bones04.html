
<!DOCTYPE html>
<html>
<head>
  <title>Bare Bones Web-LLM DeepSeek R1 Chatbot</title>
  <script type="module">
    
     // So the button can use a module function  
     window.myLoadModel  = myLoadModel
    
    import * as webllm from "https://esm.run/@mlc-ai/web-llm";

    let myEngine = new webllm.MLCEngine();

    async function myLoadModel() {
      const myModel = "DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC";
      console.log("Loading model:", myModel);
      console.log("This might take a while check Application-->cache storage --> webllm/model ");

      await myEngine.reload(myModel, { temperature: 1.0, top_p: 1 });
      console.log("Model loaded successfully!");

      const myQuestion = document.getElementById('myArea01').value
      const myMessages = [{ content: myQuestion, role: "user" }];

      await console.log("Asking:", myQuestion);
      await console.log("The answer may come up all at once after a while processing ~8 min on my machine");
      const myResponse = await myEngine.chat.completions.create({ stream: false, messages: myMessages });
      
      console.log(myResponse);
      console.log("Response:", myResponse.choices[0].message.content);
      document.getElementById('myDiv01').innerHTML =  yResponse.choices[0].message.content.replace(/\n/g, '<br>');



    }
  </script>
</head>
<body>
  <h2>Simple Web-LLM DeepSeek R1 Chatbot</h2>
  Open console using Ctrl-Shift-i<br>
  Watch Console and Application-->cache storage --> webllm/model to see the files loading<br>
  Actually uses the Web LLM MLC model "DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC" <br><br>
  
  <textarea id="myArea01" rows="5" cols="70" >What are the roots of the quadratic equation x^2 â€“ 45x + 324 = 0. </textarea><br>
  <input type="button" value="Load Model and Ask Question" onclick="myLoadModel()"><br>
  <div id="myDiv01" >...</div>
</body>
</html>
