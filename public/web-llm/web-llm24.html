
<!DOCTYPE html>
<html>

  <head>
    <title>Single Page Chatbot</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="UTF-8" />


  <script type="module">
  //  import { CreateMLCEngine, MLCEngine, prebuiltAppConfig } from "https://esm.run/@mlc-ai/web-llm";
    import * as webllm from "https://esm.run/@mlc-ai/web-llm";



/*************** WebLLM logic ***************/
const messages = [{ content: "You are a helpful AI agent helping users.", role: "system", }, ];

const availableModels = webllm.prebuiltAppConfig.model_list.map( (m) => m.model_id, );
//let selectedModel = "Llama-3-8B-Instruct-q4f32_1-MLC-1k";
let selectedModel = "Llama-3.2-1B-Instruct-q4f16_1-MLC";

// Callback function for initializing progress
function updateEngineInitProgressCallback(report) {
  console.log("initialize", report.progress);
  document.getElementById("chat-stats").textContent = report.text;
}

// Create engine instance
const engine = new webllm.MLCEngine();
engine.setInitProgressCallback(updateEngineInitProgressCallback);

async function initializeWebLLMEngine() {
  document.getElementById("chat-stats").classList.remove("hidden");
  selectedModel = document.getElementById("model-selection").value;
  const config = { temperature: 1.0, top_p: 1, };
  await engine.reload(selectedModel, config);
}

async function streamingGenerating(messages, onUpdate, onFinish, onError) {
  try {
    let curMessage = "";
    const completion = await engine.chat.completions.create({ stream: true, messages, });
    for await (const chunk of completion) {
      const curDelta = chunk.choices[0].delta.content;
      if (curDelta) {
        curMessage += curDelta;
      }
      onUpdate(curMessage);
    }
    const finalMessage = await engine.getMessage();
    onFinish(finalMessage);
  } catch (err) {
    onError(err);
  }
}

/*************** UI logic ***************/
function onMessageSend() {
  const input = document.getElementById("user-input").value.trim();
  const message = { content: input, role: "user", };
  if (input.length === 0) {
    return;
  }
//  document.getElementById("send").disabled = true;
  document.getElementById("send").value = "stop";

  messages.push(message);
  appendMessage(message);

  document.getElementById("user-input").value = "";
  document.getElementById("user-input").setAttribute("placeholder", "Generating...");

  const aiMessage = {content: "typing...", role: "assistant", };
  appendMessage(aiMessage);

  const onFinishGenerating = (finalMessage) => {
    updateLastMessage(finalMessage);
    document.getElementById("send").disabled = false;
    engine.runtimeStatsText().then(statsText => {
      document.getElementById('chat-stats').classList.remove('hidden');
      //  document.getElementById('chat-stats').textContent = statsText;
      document.getElementById('chat-stats').innerHTML = statsText;
      document.getElementById("user-input").setAttribute("placeholder", "Type a message...");
    })
  };

  streamingGenerating( messages, updateLastMessage, onFinishGenerating, console.error, );
}

function appendMessage(message) {
  const chatBox = document.getElementById("chat-box");
  const container = document.createElement("div");
  container.classList.add("message-container");
  const newMessage = document.createElement("div");
  newMessage.classList.add("message");
  newMessage.textContent = message.content;

  if (message.role === "user") {
    container.classList.add("user");
  } else {
    container.classList.add("assistant");
  }

  container.appendChild(newMessage);
  chatBox.appendChild(container);
  chatBox.scrollTop = chatBox.scrollHeight; // Scroll to the latest message
}

function updateLastMessage(content) {
  const messageDoms = document.getElementById("chat-box").querySelectorAll(".message");
  const lastMessageDom = messageDoms[messageDoms.length - 1];
  lastMessageDom.textContent = content;
}

/*************** UI binding ***************/
availableModels.forEach((modelId) => {
  const option = document.createElement("option");
  option.value = modelId;
  option.textContent = modelId;
  document.getElementById("model-selection").appendChild(option);
});
document.getElementById("model-selection").value = selectedModel;
document.getElementById("download").addEventListener("click", function () {
  initializeWebLLMEngine().then(() => {
    document.getElementById("send").disabled = false;
  });
});
document.getElementById("send").addEventListener("click", function () {
onMessageSend();
});
 document.getElementById("download-to-local-folder").addEventListener("click", function () {
  onDownToLocal();
});
document.getElementById("Load-from-local-folder").addEventListener("click", function () {
  onLoadFromLocal();
});




/* ----------------------------- load and unload model ---------------------------------------*/    

    async function onDownToLocal() {
  try {
    const selectedModel = document.getElementById("model-selection").value;
  //  const response = await fetch(`https://esm.run/@mlc-ai/web-llm/models/${selectedModel}`);
    const response = await fetch(`https://huggingface.co/mlc-ai/${selectedModel}`);


    //https://huggingface.co/mlc-ai/
    if (!response.ok) {
      throw new Error("Failed to download the model. Please check the selected model and your connection.");
    }
    const modelBlob = await response.blob();

    const downloadLink = document.createElement("a");
    downloadLink.href = URL.createObjectURL(modelBlob);
    downloadLink.download = `${selectedModel}.mlc`;
    downloadLink.click();
    URL.revokeObjectURL(downloadLink.href);

    alert("Model downloaded successfully! Save it to a folder for later use.");
  } catch (err) {
    console.error("Error in onDownToLocal:", err);
    alert("Failed to download the model. Please try again.");
  }
}

async function onLoadFromLocal() {
  try {
    const fileInput = document.getElementById("myLoadFile");
    const selectedFile = fileInput.files[0];
    if (!selectedFile) {
      alert("Please select a file to load.");
      return;
    }

    const fileData = await selectedFile.arrayBuffer();
    const config = { temperature: 1.0, top_p: 1 }; // Keep consistent with initialization

    // Reload the model into the engine
    await engine.reloadFromBuffer(fileData, config);
    alert("Model loaded and activated successfully!");
    document.getElementById("send").disabled = false; // Enable the send button after activation
  } catch (err) {
    console.error("Error in onLoadFromLocal:", err);
    alert("Failed to load the model. Please ensure it is a valid model file.");
  }
}

    
  </script>
  </head>

  <body>
    <h2 align=center>Single Page Application Web-LLM</h2>
    Step 1: Initialize WebLLM and Download Model from https://esm.run/@mlc-ai/web-llm<br>
    Unfortunately I can't find out how big some of these files are, so be careful if you are running on data. Many of these will not work with a cell phone.<br><br>
    <div class="download-container">
      <select id="model-selection"></select>
      <button id="download">Download Model to Webpage</button>
      <button id="download-to-local-folder">Download to local folder</button>
      <input type=file id="myLoadFile">
      <button id="Load-from-local-folder">Load from local folder</button>
    </div>
    <p id="download-status" class="hidden"></p>

    <div class="chat-container">
      <div id="chat-stats" ></div>
      <div class="chat-input-container">
      
      <textarea id="user-input" rows=3 cols=70 WRAP placeholder="Type a message..."></textarea>
     <!--   <input type="text" id="user-input" placeholder="Type a message..." onkeydown="{   if (event.keyCode === 13) {document.getElementById('send').click();} }" />  
          <input type="text" id="user-input" placeholder="Type a message..." />
        -->
        <button id="send" disabled>Send</button>
      </div>
      
      <div id="chat-box" class="chat-box"></div>
    </div>


  </body>
</html>
