<!DOCTYPE html>
<html>

<head>
<title>Client-Side Single HTML Javascript Application Chatbot</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta charset="UTF-8" />


<script type="module">
//  import { CreateMLCEngine, MLCEngine, prebuiltAppConfig } from "https://esm.run/@mlc-ai/web-llm";
import * as webllm from "https://esm.run/@mlc-ai/web-llm";



/*************** WebLLM logic ***************/


/********************* Globals ********************************/    

    // so these module funcitons can be used by onClick events
window.onMessageSend = onMessageSend
window.onShow = onShow
window.mySingleShow = mySingleShow
window.initializeWebLLMEngine = initializeWebLLMEngine
    

let myClear = true
let myUrlPassed = ''
    
const messages = [{ content: "You are a helpful AI agent helping users.", role: "system", }, ];

const availableModels = webllm.prebuiltAppConfig.model_list.map( (m) => m.model_id, );
//let selectedModel = "Llama-3-8B-Instruct-q4f32_1-MLC-1k";
let selectedModel = "Llama-3.2-1B-Instruct-q4f16_1-MLC";
document.getElementById('myManualModel').value = selectedModel   

  async function onShowFileCount(myUrlPassed2) {
    myUrlPassed = myUrlPassed2;
  //  console.log(myUrlPassed)
    if ( myUrlPassed2.endsWith('-1k') || myUrlPassed2.endsWith('-b4') ){
    myUrlPassed = myUrlPassed2.substr(0, myUrlPassed2.length - 3)
    //  console.log(myUrlPassed)
    }
    if ( myUrlPassed2.endsWith('-b32')  ){
    myUrlPassed = myUrlPassed2.substr(0, myUrlPassed2.length - 4)
    //  console.log(myUrlPassed)
    }
     // const myUrl = 'https://huggingface.co/api/models/mlc-ai/Llama-3.2-1B-Instruct-q4f16_1-MLC';
   const myUrl = 'https://huggingface.co/api/models/mlc-ai/'+ myUrlPassed
   // console.log(myUrl)
      try {
          const response = await fetch(myUrl);   // use this one
       //  const response  = await fetchModelMetadata(myUrl); // Fetch metadata for the selected model
       //   console.log(response)
         // const pipelineType = metadata?.pipeline || "LLMChatPipeline"; /
        //  console.log('metadata')
        //  console.log(metadata)
        /*
          if (!response.ok) {
              document.getElementById('myDiv').textContent = `Error: ${response.status}`;
              return;
          } */
          const myData = await response.json();
         // console.log('myData')
        //  console.log(myData)
          const myFiles = myData.siblings || []; // 'siblings' contains the list of files in the repository
          const myFileCount = myFiles.length;
          const description = myData.cardData.base_model || "Description not available.";

       //   document.getElementById('chat-box').textContent = `File count: ${myFileCount}`;
       // console.log(description)
       // return `${myFileCount} : ${description}`
        return myFileCount
      } catch (error) {
         // document.getElementById('chat-box').textContent = `Error: ${error.message}`;
        return   '0' // error.message
      }
  }

async function mySingleShow(myThis){
    document.getElementById('myManualModel').value = myThis;
    
    let myTemp = ''
    const myCountFiles = await onShowFileCount(myThis)
    myTemp += ` <a  target='myNewPage' rel='noopener'  href='https://huggingface.co/mlc-ai/${myUrlPassed}'>${myThis}</a> `
    myTemp += myCountFiles
    myTemp += ' files<br>'
    document.getElementById('myShowUrlDiv').innerHTML = myTemp;
}
    

async function onShow(){    
     // https://huggingface.co/mlc-ai/Llama-3.2-1B-Instruct-q4f16_1-MLC 
    // https://huggingface.co/api/models/mlc-ai/Llama-3.2-1B-Instruct-q4f16_1-MLC
// let myWeird = fetch('https://huggingface.co/api/models/mlc-ai/Llama-3.2-1B-Instruct-q4f16_1-MLC')
//  console.log(myWeird)
let myTemp = ''
for (let myLoop = 0;  myLoop < availableModels.length; myLoop++){
  const myCountFiles = await onShowFileCount(availableModels[myLoop])
  myTemp += `${myLoop} : <a onclick="alert('hi')" target='myNewPage' rel='noopener'  href='https://huggingface.co/mlc-ai/${myUrlPassed}'>${availableModels[myLoop]}</a> `
  myTemp += myCountFiles
  myTemp += ' files<br>'
  document.getElementById('chat-box').innerHTML = myTemp;
}
//document.getElementById('chat-box').innerHTML = myTemp;
}
   
    
// Callback function for initializing progress
function updateEngineInitProgressCallback(report) {
  console.log("initialize", report.progress);
  document.getElementById("chat-stats").textContent = report.text;
}

// Create engine instance
const engine = new webllm.MLCEngine();
engine.setInitProgressCallback(updateEngineInitProgressCallback);

async function initializeWebLLMEngine() {
  document.getElementById("chat-stats").classList.remove("hidden");
  //selectedModel = document.getElementById("model-selection").value;
  selectedModel = document.getElementById("myManualModel").value;


  let pipelineType = "LLMChatPipeline"; // Default pipeline
    /*
  // Determine pipeline type based on model name

  if (selectedModel.startsWith("snowflake-arctic")) {
    pipelineType = "LLMEmbedPipeline"; // Example alternative pipeline
  }


    */
  // Set the configuration for the model
  let config = { temperature: 1.0, top_p: 1, pipeline: pipelineType };

  // set config for different models
 // let config = { temperature: 1.0, top_p: 1, };   // this is what is needed

 // if ( selectedModel.endsWith('-b4') || selectedModel.endsWith('-b32') ) {
//     config = { temperature: 1.0, top_p: 1, initProgressCallback: updateEngineInitProgressCallback, logLevel: 'INFO', }
  await engine.reload(selectedModel, config);
}







    
async function streamingGenerating(messages, onUpdate, onFinish, onError) {
  try {
    let curMessage = "";
    const completion = await engine.chat.completions.create({ stream: true, messages, });
    for await (const chunk of completion) {
      const curDelta = chunk.choices[0].delta.content;
      if (curDelta) {
        curMessage += curDelta;
      }
      onUpdate(curMessage);
    }
    const finalMessage = await engine.getMessage();
    onFinish(finalMessage);
  } catch (err) {
    onError(err);
  }
}

/*************** UI logic ***************/
function onMessageSend() {
  if (myClear){
    document.getElementById("chat-box").textContent = ''
  }
  const input = document.getElementById("user-input").value.trim();
  const message = { content: input, role: "user", };
  if (input.length === 0) {
    return;
  }
//  document.getElementById("send").disabled = true;
  document.getElementById("send").value = "stop";

  messages.push(message);
  appendMessage(message);

  document.getElementById("user-input").value = "";
  document.getElementById("user-input").setAttribute("placeholder", "Generating...");

  const aiMessage = {content: "typing...", role: "assistant", };
  appendMessage(aiMessage);

  const onFinishGenerating = (finalMessage) => {
    updateLastMessage(finalMessage);
    document.getElementById("send").disabled = false;
    engine.runtimeStatsText().then(statsText => {   // soon to be deprecated
  //  engine.ChatCompletionChunk().then(statsText => {
    
      //
      document.getElementById('chat-stats').classList.remove('hidden');
      //  document.getElementById('chat-stats').textContent = statsText;
      document.getElementById('chat-stats').innerHTML = statsText;
      document.getElementById("user-input").setAttribute("placeholder", "Type a message...");
    })
  };

  streamingGenerating( messages, updateLastMessage, onFinishGenerating, console.error, );
}

function appendMessage(message) {
  const chatBox = document.getElementById("chat-box");
  const container = document.createElement("div");
  container.classList.add("message-container");
  const newMessage = document.createElement("div");
  newMessage.classList.add("message");
  newMessage.textContent = message.content;

  if (message.role === "user") {
    container.classList.add("user");
  } else {
    container.classList.add("assistant");
  }

  container.appendChild(newMessage);
  chatBox.appendChild(container);
  chatBox.scrollTop = chatBox.scrollHeight; // Scroll to the latest message
}

function updateLastMessage(content) {
  const messageDoms = document.getElementById("chat-box").querySelectorAll(".message");
  const lastMessageDom = messageDoms[messageDoms.length - 1];
  lastMessageDom.textContent = content;
}

/*************** UI binding ***************/
availableModels.forEach((modelId) => {
  const option = document.createElement("option");
  option.value = modelId;
  option.textContent = modelId;
  document.getElementById("model-selection").appendChild(option);
});
document.getElementById("model-selection").value = selectedModel;

    /*
document.getElementById("show").addEventListener("click", function () {
  onShow();
});

  */
document.getElementById("download").addEventListener("click", function () {
  document.getElementById("send").disabled = true;
  initializeWebLLMEngine().then(() => {
    document.getElementById("send").disabled = false;
  });
});

  /*
document.getElementById("send").addEventListener("click", function () {
  onMessageSend();
});

  */
    
</script>



</head>

<body>
  
<h2 align=center>HTML Javascript Single Page Application Web-LLM Chatbot Fully Client-Side</h2>
Resources: API from https://esm.run/@mlc-ai/web-llm main Web-LLM github models <a href="https://github.com/mlc-ai">https://github.com/mlc-ai</a><br>
Generally these LLM's will not run on your cell phone, even a desktop computer will benefit from dual NVIDIA GPU's<br>
These Github webpages is at <a href="https://github.com/hpssjellis/my-examples-of-ai-agents">my-examples-of-ai-agents</a><br><br>
To convert LLM models to WEB_LLM MLC format by converting the weights
<a href="https://llm.mlc.ai/docs/compilation/convert_weights.html">https://llm.mlc.ai/docs/compilation/convert_weights.html</a><br><br>

<button id="show" onclick="onShow()">Show these models</button>
<select id="model-selection" onChange="{ mySingleShow(this.value) }"></select><br>
<input id="myManualModel" type=text value="" size=80><br>
  <div id="myShowUrlDiv">...</div><br>
<button id="download"  style="background-color:lime;">Download Model or Auto load from Local Cache</button>


<p id="download-status" class="hidden">...</p>


<div id="chat-stats" ></div>


<textarea id="user-input" rows=3 cols=70 WRAP placeholder="Type a message..."></textarea>
<!--   <input type="text" id="user-input" placeholder="Type a message..." onkeydown="{   if (event.keyCode === 13) {document.getElementById('send').click();} }" />  
  <input type="text" id="user-input" placeholder="Type a message..." />
-->
<button id="send" disabled onclick="{onMessageSend()}">Send</button>


<div id="chat-box" class="chat-box">...</div> <br><br>

    Use at your own (download data) risk: By Jeremy Ellis, Linkedin: <a href="https://www.linkedin.com/in/jeremy-ellis-4237a9bb/">https://www.linkedin.com/in/jeremy-ellis-4237a9bb/</a>
  </body>
</html>
